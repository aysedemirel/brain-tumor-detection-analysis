{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":377107,"sourceType":"datasetVersion","datasetId":165566}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Brain Tumor Detection using MRI Images","metadata":{}},{"cell_type":"markdown","source":"## Project Overview\nThis project focuses on detecting brain tumors from MRI images using deep learning techniques. The main goals are:\n\n- Build a robust image classification model to distinguish between tumor-positive and tumor-negative MRI scans.\n- Experiment with both **custom CNNs** and **pre-trained models** (EfficientNetB0, MobileNetV2, ResNet50, VGG16, InceptionV3) using **transfer learning** and **fine-tuning**.\n- Apply **data augmentation** (crop, rotate, flip, shift, brightness adjustments, noise, zoom) to increase dataset size and improve model generalization.\n- Monitor and mitigate overfitting using **EarlyStopping** and validation data.\n- Optimize hyperparameters with **Keras Tuner** to improve model performance.\n\n## Dataset\n- Dataset: `brain-mri-images-for-brain-tumor-detection` from Kaggle\n- Classes: `YES` (tumor), `NO` (no tumor)\n- Original dataset size: 193 training images per class\n- Augmented dataset size: 1544 images (after crop, rotate, flip, shift, brightness, noise, zoom)\n\n## Methodology\n1. **Data Preprocessing**\n   - Images resized to `224x224`\n   - Pixel normalization\n   - Data augmentation to increase training data and reduce overfitting\n\n2. **Modeling**\n   - Experimented with **pre-trained CNNs** and **custom CNN**\n   - Applied **transfer learning** and fine-tuning for best performance\n   - Used **dropout** and **L2 regularization** to mitigate overfitting\n\n3. **Training & Evaluation**\n   - Used `binary_crossentropy` loss and `Adam` optimizer\n   - Monitored validation accuracy for early stopping\n   - Hyperparameter tuning performed with **Keras Tuner**\n\n## Tools & Libraries\n- Python, TensorFlow / Keras\n- OpenCV for image processing\n- Matplotlib for visualization\n- Keras Tuner for hyperparameter optimization","metadata":{}},{"cell_type":"markdown","source":"## Import dependencies","metadata":{}},{"cell_type":"code","source":"from IPython.display import clear_output\n# !pip install imutils\n# !pip install --upgrade tensorflow\n# !pip install scikeras\nclear_output()\n\n#import tensorflow as tf\n#print(tf.__version__)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install scikit-learn==1.3.2\n!pip install scikit-optimize --upgrade","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport keras_tuner as kt\nimport numpy as np\nimport os\nimport cv2\nimport shutil\nimport itertools\nimport imutils\nimport random\nimport matplotlib.pyplot as plt\n\nfrom tqdm.notebook import tqdm\nfrom scipy.ndimage import map_coordinates, gaussian_filter\nfrom scikeras.wrappers import KerasClassifier\n\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\nfrom skopt import gp_minimize\nfrom skopt.space import Real, Integer\nfrom skopt.utils import use_named_args\n\n\nfrom tensorflow.keras import layers, regularizers, Sequential\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.optimizers import Adam, RMSprop\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import EfficientNetB0, MobileNetV2, DenseNet169, ResNet50, VGG16, InceptionV3\nfrom tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n\n\nRANDOM_SEED = 123\nIMG_SIZE = (224,224)\nTRAIN_MERGE_DIR = 'TRAIN_MERGED/'\nVAL_MERGE_DIR = 'VAL_MERGED/'\nTEST_MERGE_DIR = 'TEST_MERGED/'\nTRAIN_DIR = 'TRAIN/'\nTEST_DIR = 'TEST/'\nVAL_DIR = 'VAL/'\n\n#import warnings\n#warnings.filterwarnings('default')  # show all warnings","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataset Preparation\n\nThis section organizes the raw MRI images into **training, validation, and test sets**. \n\n- Each class (`YES` for tumor, `NO` for no tumor) is separated into its respective folder.\n- Split:\n  - **Training:** 80% of the data\n  - **Validation:** 20% of the remaining data after training\n  - **Test:** First 5 images of each class\n- This ensures that the model can be trained, validated, and tested on separate subsets to prevent data leakage and evaluate generalization.\n- The code automatically creates the folder structure and copies images accordingly.","metadata":{}},{"cell_type":"code","source":"IMG_PATH = \"/kaggle/input/brain-mri-images-for-brain-tumor-detection/brain_tumor_dataset/\"\n\n# create folders\nfor split in ['TRAIN', 'VAL', 'TEST']:\n    for class_name in ['YES', 'NO']:\n        os.makedirs(f'{split}/{class_name}', exist_ok=True)\n\n# split dataset\nfor CLASS in os.listdir(IMG_PATH):\n    if not CLASS.startswith('.'):\n        IMG_NUM = len(os.listdir(IMG_PATH + CLASS))\n        for (n, FILE_NAME) in enumerate(os.listdir(IMG_PATH + CLASS)):\n            img = IMG_PATH + CLASS + '/' + FILE_NAME\n            if n < 5:\n                shutil.copy(img, 'TEST/' + CLASS.upper() + '/' + FILE_NAME)\n            elif n < 0.8*IMG_NUM:\n                shutil.copy(img, 'TRAIN/'+ CLASS.upper() + '/' + FILE_NAME)\n            else:\n                shutil.copy(img, 'VAL/'+ CLASS.upper() + '/' + FILE_NAME)\n\n# print results\nfor split in ['TRAIN', 'VAL', 'TEST']:\n    for class_name in ['YES', 'NO']:\n        path = f'{split}/{class_name}'\n        num_files = len(os.listdir(path))\n        print(f\"{split}/{class_name} has {num_files} images\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_data(data_dir, img_size):\n    X = []\n    y = []\n    labels = []\n    \n    for class_name in os.listdir(data_dir):\n        if class_name.startswith('.'):\n            continue\n        class_path = os.path.join(data_dir, class_name)\n        labels.append(class_name)\n        for img_file in os.listdir(class_path):\n            img_path = os.path.join(class_path, img_file)\n            img = cv2.imread(img_path)          \n            if img is None:\n                print(\"None: \", img)\n                continue                        \n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n            img = cv2.resize(img, img_size)    \n            X.append(img)\n            y.append(class_name)\n    \n    X = np.array(X, dtype='float32')\n    y = np.array(y)\n    \n    # Label encode\n    lb = LabelBinarizer()\n    y = lb.fit_transform(y)\n    \n    return X, y, lb.classes_\n\n# use predefined function to load the image data into workspace\nX_train, y_train, labels = load_data(TRAIN_DIR, IMG_SIZE)\nX_test, y_test, _ = load_data(TEST_DIR, IMG_SIZE)\nX_val, y_val, _ = load_data(VAL_DIR, IMG_SIZE)\n\nprint(\"X_train shape:\", X_train.shape)\nprint(\"y_train shape:\", y_train.shape)\nprint(\"X_val shape:\", X_val.shape)\nprint(\"y_val shape:\", y_val.shape)\nprint(\"X_test shape:\", X_test.shape)\nprint(\"y_test shape:\", y_test.shape)\nprint(\"Labels:\", labels)\n\n\nplt.imshow(X_train[0].astype('uint8'))\nplt.title(f\"Label: {labels[np.argmax(y_train[0])]}\")\nplt.axis('off')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_samples(X, y, labels=None, n_samples=25):\n    \"\"\"\n    X: image data (numpy array)\n    y: one-hot or encoded label array\n    labels: class names (optional)\n    n_samples: number of samples to display\n    \"\"\"\n    plt.figure(figsize=(12,12))\n    indices = np.random.choice(len(X), n_samples, replace=False)\n    for i, idx in enumerate(indices):\n        plt.subplot(int(np.sqrt(n_samples))+1, int(np.sqrt(n_samples))+1, i+1)\n        plt.imshow(X[idx].astype('uint8'))\n        plt.axis('off')\n        if labels is not None:\n            plt.title(labels[np.argmax(y[idx])])\n    plt.tight_layout()\n    plt.show()\n    \nplot_samples(X_train, y_train, labels, 30)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Image Preprocessing: Crop Example\n\nA sample MRI image is cropped to focus on the tumor region. Steps:\n1. Convert to RGB and apply Gaussian blur to reduce noise.\n2. Apply thresholding and erosion/dilation to highlight the main tumor area.\n3. Detect contours and select the largest one.\n4. Find extreme points and crop the image.\n5. Cropped image is ready for model training.\n\n> This is a demo for cropping; the same process will be applied to the full dataset.","metadata":{}},{"cell_type":"code","source":"img = cv2.imread('../input/brain-mri-images-for-brain-tumor-detection/brain_tumor_dataset/yes/Y108.jpg')\nimg = cv2.resize(\n            img,\n            dsize=IMG_SIZE,\n            interpolation=cv2.INTER_CUBIC\n        )\ngray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\ngray = cv2.GaussianBlur(gray, (5, 5), 0)\n\n# threshold the image, then perform a series of erosions +\n# dilations to remove any small regions of noise\nthresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\nthresh = cv2.erode(thresh, None, iterations=2)\nthresh = cv2.dilate(thresh, None, iterations=2)\n\n# find contours in thresholded image, then grab the largest one\ncnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\ncnts = imutils.grab_contours(cnts)\nc = max(cnts, key=cv2.contourArea)\n\n# find the extreme points\nextLeft = tuple(c[c[:, :, 0].argmin()][0])\nextRight = tuple(c[c[:, :, 0].argmax()][0])\nextTop = tuple(c[c[:, :, 1].argmin()][0])\nextBot = tuple(c[c[:, :, 1].argmax()][0])\n\n# add contour on the image\nimg_cnt = cv2.drawContours(img.copy(), [c], -1, (0, 255, 255), 4)\n\n# add extreme points\nimg_pnt = cv2.circle(img_cnt.copy(), extLeft, 8, (0, 0, 255), -1)\nimg_pnt = cv2.circle(img_pnt, extRight, 8, (0, 255, 0), -1)\nimg_pnt = cv2.circle(img_pnt, extTop, 8, (255, 0, 0), -1)\nimg_pnt = cv2.circle(img_pnt, extBot, 8, (255, 255, 0), -1)\n\n# crop\nADD_PIXELS = 0\nnew_img = img[extTop[1]-ADD_PIXELS:extBot[1]+ADD_PIXELS, extLeft[0]-ADD_PIXELS:extRight[0]+ADD_PIXELS].copy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(15,6))\nplt.subplot(141)\nplt.imshow(img)\nplt.xticks([])\nplt.yticks([])\nplt.title('Step 1. Get the original image')\nplt.subplot(142)\nplt.imshow(img_cnt)\nplt.xticks([])\nplt.yticks([])\nplt.title('Step 2. Find the biggest contour')\nplt.subplot(143)\nplt.imshow(img_pnt)\nplt.xticks([])\nplt.yticks([])\nplt.title('Step 3. Find the extreme points')\nplt.subplot(144)\nplt.imshow(new_img)\nplt.xticks([])\nplt.yticks([])\nplt.title('Step 4. Crop the image')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Augmentation\n\nIn order to increase the size and diversity of our brain MRI dataset, several data augmentation techniques have been applied. These augmentations help the model generalize better and reduce overfitting.\n\n### Applied Augmentation Methods\n\n1. **Cropping**\n   - Extracted the region of interest (brain area) from each image.\n   - Images were resized to `(224, 224)` after cropping.\n   - Helps focus the model on relevant areas and remove unnecessary background.\n\n2. **Rotation**\n   - Random rotations applied within a specified range (e.g., ±30 degrees).\n   - Rotation was applied on the cropped images to preserve the ROI.\n   - Increases orientation invariance of the model.\n\n3. **Flipping**\n   - Horizontal and vertical flips applied.\n   - Introduces mirror images to the dataset, improving robustness.\n\n4. **Shifting**\n   - Random translations along x and y axes (e.g., 10% of image size).\n   - Makes the model less sensitive to object positions.\n\n5. **Zooming**\n   - Zoom in (factor > 1) or zoom out (factor < 1).\n   - Zooming in crops the image centrally, zooming out pads the image with black borders.\n   - Increases scale invariance.\n\n6. **Brightness & Contrast Adjustment**\n   - Random adjustments applied to brightness and contrast.\n   - Simulates different lighting conditions.\n\n7. **Adding Noise**\n   - Gaussian noise added with specified mean and standard deviation.\n   - Helps the model learn to ignore small artifacts.\n\n8. **Elastic Deformation**\n   - Applies smooth, random local deformations to the images.\n   - Improves model robustness to small shape variations.\n\n### Summary\n\n- Starting dataset: 193 images per class.  \n- After all augmentation techniques: 1737 images in the training set.  \n- Validation and test sets were also augmented separately for robustness checks.  \n- All images were resized to `(224, 224)` and maintained RGB channels.\n\nThese augmentations were implemented using OpenCV, NumPy, and SciPy, ensuring a wide range of transformations while preserving the essential features of the brain MRI scans.","metadata":{}},{"cell_type":"code","source":"def crop_imgs(set_name, add_pixels_value=0, img_size=(224,224)):\n    set_new = []\n    for img in set_name:\n        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n        gray = cv2.GaussianBlur(gray, (5, 5), 0)\n\n        thresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\n        thresh = thresh.astype(np.uint8)\n        thresh = cv2.erode(thresh, None, iterations=2)\n        thresh = cv2.dilate(thresh, None, iterations=2)\n\n        cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        cnts = imutils.grab_contours(cnts)\n        c = max(cnts, key=cv2.contourArea)\n\n        extLeft = max(0, c[c[:, :, 0].argmin()][0][0])\n        extRight = min(img.shape[1], c[c[:, :, 0].argmax()][0][0])\n        extTop = max(0, c[c[:, :, 1].argmin()][0][1])\n        extBot = min(img.shape[0], c[c[:, :, 1].argmax()][0][1])\n\n        ADD_PIXELS = add_pixels_value\n        new_img = img[extTop-ADD_PIXELS:extBot+ADD_PIXELS, extLeft-ADD_PIXELS:extRight+ADD_PIXELS].copy()\n\n        # Resize to a single size\n        new_img = cv2.resize(new_img, img_size)\n        set_new.append(new_img)\n    return np.array(set_new)\n\n# apply this for each set\nX_train_crop = crop_imgs(set_name=X_train)\nX_val_crop = crop_imgs(set_name=X_val)\nX_test_crop = crop_imgs(set_name=X_test)\n\nprint(\"Cropped shapes:\", X_train_crop.shape, X_val_crop.shape, X_test_crop.shape)\nplot_samples(X_train_crop, y_train, labels, 30)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def rotate_images(set_name, img_size=(224,224), rotation_range=30):\n    \"\"\"\n    Only rotates images that have been rotated.\n    set_name: numpy array image list (RGB)\n    img_size: output size\n    rotation_range: random rotation between -rotation_range and +rotation_range\n    \"\"\"\n    set_new = []\n    for img in set_name:\n        # Select random angle\n        angle = np.random.uniform(-rotation_range, rotation_range)\n        # Rotate\n        rotated_img = imutils.rotate_bound(img, angle)\n        # Fix size\n        rotated_img = cv2.resize(rotated_img, img_size)\n        set_new.append(rotated_img)\n        \n    return np.array(set_new)\n\n# Apply random rotation to the cropped images to augment the dataset\n# This increases data diversity while keeping the tumor region centered\nX_train_rotate = rotate_images(set_name=X_train_crop)\nX_val_rotate = rotate_images(set_name=X_val_crop)\nX_test_rotate = rotate_images(set_name=X_test_crop)\n\nprint(\"Rotated shapes:\", \n      X_train_rotate.shape, \n      X_val_rotate.shape, \n      X_test_rotate.shape)\nplot_samples(X_train_rotate, y_train, labels, 30)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def flip_images(set_name, mode='horizontal', img_size=(224,224)):\n    \"\"\"\n    mode: 'horizontal' (right–left), 'vertical' (up–down)\n    \"\"\"\n    set_new = []\n    for img in set_name:\n        if mode == 'horizontal':\n            flipped = cv2.flip(img, 1)   # left-right\n        elif mode == 'vertical':\n            flipped = cv2.flip(img, 0)   # up-down\n        else:\n            raise ValueError(\"mode must be 'horizontal' or 'vertical'\")\n        flipped = cv2.resize(flipped, img_size)\n        set_new.append(flipped)\n    return np.array(set_new)\n\n# Apply horizontal flip on cropped images to increase dataset diversity\nX_train_flip = flip_images(set_name=X_train_crop, mode='horizontal')\nX_val_flip = flip_images(set_name=X_val_crop, mode='horizontal')\nX_test_flip = flip_images(set_name=X_test_crop, mode='horizontal')\n\nprint(\"Fliped shapes:\", \n      X_train_flip.shape, \n      X_val_flip.shape, \n      X_test_flip.shape)\nplot_samples(X_train_flip, y_train, labels, 30)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def shift_images(set_name, x_shift=0.1, y_shift=0.1, img_size=(224,224)):\n    \"\"\"\n    set_name : numpy array\n        List of images (RGB)\n    x_shift, y_shift : float\n        Fraction of total width/height to shift (e.g., 0.1 = 10%)\n    img_size : tuple\n        Output image size (height, width)\n    \"\"\"\n    set_new = []\n    h, w = img_size\n    for img in set_name:\n        M = np.float32([[1, 0, x_shift*w], [0, 1, y_shift*h]])\n        shifted = cv2.warpAffine(img, M, (w, h))\n        set_new.append(shifted)\n    return np.array(set_new)\n\n# Apply shift on cropped images\nX_train_shift = shift_images(set_name=X_train_crop, x_shift=0.1, y_shift=0.1)\nX_val_shift = shift_images(set_name=X_val_crop, x_shift=0.1, y_shift=0.1)\nX_test_shift = shift_images(set_name=X_test_crop, x_shift=0.1, y_shift=0.1)\n\nprint(\"Shifted shapes:\", \n      X_train_shift.shape, \n      X_val_shift.shape, \n      X_test_shift.shape)\nplot_samples(X_train_shift, y_train, labels, 30)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def zoom_images(set_name, zoom_factor=1.2, img_size=(224,224)):\n    \"\"\"\n    set_name : numpy array\n        List of images (RGB)\n    zoom_factor : float\n        >1 → zoom in, <1 → zoom out\n    img_size : tuple\n        Output image size (height, width)\n    \"\"\"\n    set_new = []\n    h, w = img_size\n    for img in set_name:\n        # Compute new size\n        new_h, new_w = int(h * zoom_factor), int(w * zoom_factor)\n        zoomed = cv2.resize(img, (new_w, new_h))\n        \n        # Zoom in → crop, Zoom out → pad\n        if zoom_factor > 1:\n            start_x = (new_w - w) // 2\n            start_y = (new_h - h) // 2\n            zoomed = zoomed[start_y:start_y+h, start_x:start_x+w]\n        else:\n            pad_x = (w - new_w) // 2\n            pad_y = (h - new_h) // 2\n            zoomed = cv2.copyMakeBorder(\n                zoomed, pad_y, pad_y, pad_x, pad_x, cv2.BORDER_CONSTANT, value=(0,0,0)\n            )\n        set_new.append(cv2.resize(zoomed, img_size))\n    return np.array(set_new)\n\n# Apply zoom on cropped images\nX_train_zoom = zoom_images(set_name=X_train_crop, zoom_factor=1.2)\nX_val_zoom = zoom_images(set_name=X_val_crop, zoom_factor=1.2)\nX_test_zoom = zoom_images(set_name=X_test_crop, zoom_factor=1.2)\n\nprint(\"Zoomed shapes:\", \n      X_train_zoom.shape, \n      X_val_zoom.shape, \n      X_test_zoom.shape)\nplot_samples(X_train_zoom, y_train, labels, 30)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def adjust_brightness_contrast(set_name, brightness=30, contrast=30, img_size=(224,224)):\n    \"\"\"\n    Adjust brightness and contrast of images.\n    set_name : numpy array\n        List of images (RGB)\n    brightness : int\n        Brightness adjustment (-100 to 100)\n    contrast : int\n        Contrast adjustment (-100 to 100)\n    img_size : tuple\n        Output image size (height, width)\n    \"\"\"\n    set_new = []\n    for img in set_name:\n        new_img = cv2.convertScaleAbs(img, alpha=1 + contrast/100, beta=brightness)\n        new_img = cv2.resize(new_img, img_size)\n        set_new.append(new_img)\n    return np.array(set_new)\n\n# Apply brightness/contrast on cropped images\nX_train_bright = adjust_brightness_contrast(set_name=X_train_crop, brightness=30, contrast=30)\nX_val_bright = adjust_brightness_contrast(set_name=X_val_crop, brightness=30, contrast=30)\nX_test_bright = adjust_brightness_contrast(set_name=X_test_crop, brightness=30, contrast=30)\n\nprint(\"Bright shapes:\", \n      X_train_bright.shape, \n      X_val_bright.shape, \n      X_test_bright.shape)\nplot_samples(X_train_bright, y_train, labels, 30)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_noise(set_name, mean=0, std=15, img_size=(224,224)):\n    \"\"\"\n    Add Gaussian noise to images.\n    set_name : numpy array\n        List of images (RGB)\n    mean : float\n        Mean of Gaussian noise\n    std : float\n        Standard deviation of Gaussian noise\n    img_size : tuple\n        Output image size (height, width)\n    \"\"\"\n    set_new = []\n    for img in set_name:\n        noise = np.random.normal(mean, std, img.shape).astype(np.float32)\n        noisy_img = np.clip(img.astype(np.float32) + noise, 0, 255).astype(np.uint8)\n        noisy_img = cv2.resize(noisy_img, img_size)\n        set_new.append(noisy_img)\n    return np.array(set_new)\n\n# Apply noise on cropped images\nX_train_noise = add_noise(set_name=X_train_crop)\nX_val_noise = add_noise(set_name=X_val_crop)\nX_test_noise = add_noise(set_name=X_test_crop)\n\nprint(\"Noise shapes:\", \n      X_train_noise.shape, \n      X_val_noise.shape, \n      X_test_noise.shape)\nplot_samples(X_train_noise, y_train, labels, 30)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def elastic_deformation(images, alpha=34, sigma=4, img_size=(224,224), random_state=None):\n    \"\"\"\n    Apply elastic deformation to a batch of images\n    images : numpy array\n        Input images (H, W, C)\n    alpha : float\n        Scaling factor for deformation intensity\n    sigma : float\n        Standard deviation for Gaussian filter (smoothness)\n    img_size : tuple\n        Output size (height, width)\n    random_state : int or None\n        Seed for reproducibility\n    \"\"\"\n    if random_state is not None:\n        np.random.seed(random_state)\n\n    h, w = img_size\n    deformed_images = []\n\n    for img in images:\n        # Convert to float32\n        img_float = img.astype(np.float32)\n\n        # Random displacement fields\n        dx = gaussian_filter((np.random.rand(h, w) * 2 - 1), sigma) * alpha\n        dy = gaussian_filter((np.random.rand(h, w) * 2 - 1), sigma) * alpha\n\n        # Create meshgrid\n        x, y = np.meshgrid(np.arange(w), np.arange(h))\n\n        # Distort indices\n        indices = np.reshape(y + dy, (-1, 1)), np.reshape(x + dx, (-1, 1))\n\n        # Apply mapping for each channel\n        deformed = np.zeros_like(img_float)\n        for i in range(img.shape[2]):\n            deformed[:,:,i] = map_coordinates(img_float[:,:,i], indices, order=1, mode='reflect').reshape(h, w)\n\n        deformed_images.append(deformed.astype(np.uint8))\n\n    return np.array(deformed_images)\n\nX_train_elastic = elastic_deformation(X_train_crop, alpha=34, sigma=4, img_size=(224,224))\nX_val_elastic = elastic_deformation(X_val_crop, alpha=34, sigma=4, img_size=(224,224))\nX_test_elastic = elastic_deformation(X_test_crop, alpha=34, sigma=4, img_size=(224,224))\n\nprint(\"Elastic shapes:\", \n      X_train_elastic.shape, \n      X_val_elastic.shape, \n      X_test_elastic.shape)\n\nplot_samples(X_train_elastic, y_train, labels, 30)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save augmented images to folders\n# X: image array (NumPy)\n# y: labels (one-hot or integer encoded)\n# labels: class names list, e.g., ['NO', 'YES']\n# folder_name: target directory for saving images\ndef save_new_images(X, y, labels, folder_name='DATA/'):\n    if labels is None:\n        raise ValueError(\"The labels parameter must be given!\")\n\n    # Handle one-hot encoded labels\n    if len(y.shape) > 1 and y.shape[1] > 1:\n        y_labels = np.argmax(y, axis=1)\n    else:\n        y_labels = y.flatten()  # flat array for single-label\n\n    for i, img in enumerate(X):\n        label = labels[y_labels[i]]  # get class name\n        save_path = os.path.join(folder_name, label)\n        os.makedirs(save_path, exist_ok=True)\n\n        # Convert image to uint8 (0-255) if not already\n        img_uint8 = img.copy()\n        if img_uint8.dtype != np.uint8:\n            img_uint8 = ((img_uint8 - img_uint8.min()) / (img_uint8.max() - img_uint8.min()) * 255).astype(np.uint8)\n\n         # OpenCV uses BGR format for saving\n        cv2.imwrite(os.path.join(save_path, f'{i}.jpg'), cv2.cvtColor(img_uint8, cv2.COLOR_RGB2BGR))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_augmented_folders = ['TRAIN_CROP', \n               'VAL_CROP', \n               'TEST_CROP',\n               'TRAIN_ROTATE', \n               'VAL_ROTATE', \n               'TEST_ROTATE',\n               'TRAIN_FLIP', \n               'VAL_FLIP', \n               'TEST_FLIP',\n               'TRAIN_SHIFT', \n               'VAL_SHIFT', \n               'TEST_SHIFT',\n               'TRAIN_ZOOM', \n               'VAL_ZOOM', \n               'TEST_ZOOM',\n               'TRAIN_BRIGHT', \n               'VAL_BRIGHT', \n               'TEST_BRIGHT',\n               'TRAIN_NOISE', \n               'VAL_NOISE', \n               'TEST_NOISE',        \n               'TRAIN_ELASTIC', \n               'VAL_ELASTIC', \n               'TEST_ELASTIC'\n              ]\n\nfor folder in all_augmented_folders:\n    if os.path.exists(folder):\n        shutil.rmtree(folder)\n        \nsave_new_images(X_train_crop, y_train, labels=labels, folder_name='TRAIN_CROP/')\nsave_new_images(X_val_crop, y_val, labels=labels, folder_name='VAL_CROP/')\nsave_new_images(X_test_crop, y_test, labels=labels, folder_name='TEST_CROP/')\n\nsave_new_images(X_train_rotate, y_train, labels=labels, folder_name='TRAIN_ROTATE/')\nsave_new_images(X_val_rotate, y_val, labels=labels, folder_name='VAL_ROTATE/')\nsave_new_images(X_test_rotate, y_test, labels=labels, folder_name='TEST_ROTATE/')\n\nsave_new_images(X_train_flip, y_train, labels=labels, folder_name='TRAIN_FLIP/')\nsave_new_images(X_val_flip, y_val, labels=labels, folder_name='VAL_FLIP/')\nsave_new_images(X_test_flip, y_test, labels=labels, folder_name='TEST_FLIP/')\n\nsave_new_images(X_train_shift, y_train, labels=labels, folder_name='TRAIN_SHIFT/')\nsave_new_images(X_val_shift, y_val, labels=labels, folder_name='VAL_SHIFT/')\nsave_new_images(X_test_shift, y_test, labels=labels, folder_name='TEST_SHIFT/')\n\nsave_new_images(X_train_zoom, y_train, labels=labels, folder_name='TRAIN_ZOOM/')\nsave_new_images(X_val_zoom, y_val, labels=labels, folder_name='VAL_ZOOM/')\nsave_new_images(X_test_zoom, y_test, labels=labels, folder_name='TEST_ZOOM/')\n\nsave_new_images(X_train_bright, y_train, labels=labels, folder_name='TRAIN_BRIGHT/')\nsave_new_images(X_val_bright, y_val, labels=labels, folder_name='VAL_BRIGHT/')\nsave_new_images(X_test_bright, y_test, labels=labels, folder_name='TEST_BRIGHT/')\n\nsave_new_images(X_train_noise, y_train, labels=labels, folder_name='TRAIN_NOISE/')\nsave_new_images(X_val_noise, y_val, labels=labels, folder_name='VAL_NOISE/')\nsave_new_images(X_test_noise, y_test, labels=labels, folder_name='TEST_NOISE/')\n\nsave_new_images(X_train_elastic, y_train, labels=labels, folder_name='TRAIN_ELASTIC/')\nsave_new_images(X_val_elastic, y_val, labels=labels, folder_name='VAL_ELASTIC/')\nsave_new_images(X_test_elastic, y_test, labels=labels, folder_name='TEST_ELASTIC/')\n\n\"\"\"for folder in all_augmented_folders:\n    print(f\"{folder} içeriği:\", os.listdir(folder))\n    for class_name in ['YES', 'NO']:\n        class_path = os.path.join(folder, class_name)\n        if os.path.exists(class_path):\n            print(f\"  {class_name} sayısı:\", len(os.listdir(class_path)))\n\"\"\"\n\nprint(\"X_train: \", X_train.shape)\nprint(\"X_train_crop: \", X_train_crop.shape)\nprint(\"X_train_rotate: \", X_train_rotate.shape)\nprint(\"X_train_flip: \", X_train_flip.shape)\nprint(\"X_train_shift: \", X_train_shift.shape)\nprint(\"X_train_zoom: \", X_train_zoom.shape)\nprint(\"X_train_bright: \", X_train_bright.shape)\nprint(\"X_train_noise: \", X_train_noise.shape)\nprint(\"X_train_elastic: \", X_train_elastic.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Merge folders","metadata":{}},{"cell_type":"code","source":"def merge_folder(folders_to_merge, merged_dir):\n    for folder in folders_to_merge:\n        for class_name in ['YES', 'NO']:\n            src_dir = os.path.join(folder, class_name)\n            dst_dir = os.path.join(merged_dir, class_name)\n            os.makedirs(dst_dir, exist_ok=True)\n            \n            for i, fname in enumerate(os.listdir(src_dir)):\n                src_path = os.path.join(src_dir, fname)\n                # benzersiz isim üret\n                new_fname = f\"{os.path.basename(folder)}_{i}_{fname}\"\n                dst_path = os.path.join(dst_dir, new_fname)\n                shutil.copy(src_path, dst_path)\n\n\ndef clean_folders(folders):\n    for folder in folders:\n        if os.path.exists(folder):\n            shutil.rmtree(folder)\n            print(f\"{folder} deleted.\")\n        else:\n            print(f\"{folder} not found\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_folders_to_merge = ['TRAIN', \n                          'TRAIN_CROP', \n                          'TRAIN_ROTATE',\n                          'TRAIN_FLIP',\n                          'TRAIN_SHIFT',\n                          'TRAIN_ZOOM',\n                          'TRAIN_BRIGHT',\n                          'TRAIN_NOISE',\n                          'TRAIN_ELASTIC'\n                         ]\n\nval_folders_to_merge = ['VAL', \n                        'VAL_CROP', \n                        'VAL_ROTATE',\n                        'VAL_FLIP',\n                        'VAL_SHIFT',\n                        'VAL_ZOOM',\n                        'VAL_BRIGHT',\n                        'VAL_NOISE',\n                        'VAL_ELASTIC'\n                       ]\n\n\ntest_folders_to_merge = ['TEST', \n                         'TEST_CROP', \n                         'TEST_ROTATE',\n                         'TEST_FLIP',\n                         'TEST_SHIFT',\n                         'TEST_ZOOM',\n                         'TEST_BRIGHT',\n                         'TEST_NOISE',\n                         'TEST_ELASTIC'\n                        ]\n\n\nif os.path.exists(TRAIN_MERGE_DIR):\n    shutil.rmtree(TRAIN_MERGE_DIR)  \n\nif os.path.exists(VAL_MERGE_DIR):\n    shutil.rmtree(VAL_MERGE_DIR) \n\nif os.path.exists(TEST_MERGE_DIR):\n    shutil.rmtree(TEST_MERGE_DIR)  \n\nmerge_folder(train_folders_to_merge, TRAIN_MERGE_DIR)\nmerge_folder(val_folders_to_merge, VAL_MERGE_DIR)\nmerge_folder(test_folders_to_merge, TEST_MERGE_DIR)\n\nclean_folders(train_folders_to_merge)\nclean_folders(val_folders_to_merge)\nclean_folders(test_folders_to_merge)\n\n\nfor class_name in ['YES', 'NO']:\n    class_dir = os.path.join(TRAIN_MERGE_DIR, class_name)\n    files = os.listdir(class_dir)\n    print(class_name, len(files))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Data Augmentation Result","metadata":{}},{"cell_type":"code","source":"merged_dirs = {\n    'Train': TRAIN_MERGE_DIR,\n    'Validation': VAL_MERGE_DIR,\n    'Test': TEST_MERGE_DIR\n}\n\ndata_counts = {}\nfor split_name, dir_path in merged_dirs.items():\n    total = 0\n    for class_name in ['YES', 'NO']:\n        class_dir = os.path.join(dir_path, class_name)\n        total += len(os.listdir(class_dir))\n    data_counts[split_name] = total\n\nlabels = list(data_counts.keys())\nsizes = list(data_counts.values())\ncolors = ['#66b3ff', '#99ff99', '#ffcc99']\n\nplt.figure(figsize=(7,7))\nplt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)\nplt.title(\"Train / Validation / Test Dataset Distribution\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train Data","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.applications.vgg16 import preprocess_input as vgg16_preprocess\nfrom tensorflow.keras.applications.efficientnet import preprocess_input as effnet_preprocess\nfrom tensorflow.keras.applications.resnet50 import preprocess_input as resnet_preprocess\nfrom tensorflow.keras.applications.mobilenet_v2 import preprocess_input as mobilenet_preprocess\nfrom tensorflow.keras.applications.densenet import preprocess_input as densenet_preprocess\nfrom tensorflow.keras.applications.inception_v3 import preprocess_input as inception_preprocess\n\ndef get_generators(train_dir, val_dir, img_size=(224,224), batch_size=32, seed=123, preprocess_func=None):\n    train_datagen = ImageDataGenerator(\n        rotation_range=15,\n        width_shift_range=0.1,\n        height_shift_range=0.1,\n        shear_range=0.1,\n        brightness_range=[0.5, 1.5],\n        horizontal_flip=True,\n        vertical_flip=True,\n        preprocessing_function=preprocess_func\n    )\n    \n    test_datagen = ImageDataGenerator(preprocessing_function=preprocess_func)\n    \n    train_gen = train_datagen.flow_from_directory(\n        train_dir,\n        target_size=img_size,\n        batch_size=batch_size,\n        class_mode='binary',\n        seed=seed\n    )\n    \n    val_gen = test_datagen.flow_from_directory(\n        val_dir,\n        target_size=img_size,\n        batch_size=batch_size//2,\n        class_mode='binary',\n        seed=seed\n    )\n    \n    return train_gen, val_gen","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The following method (`` plot_training_history ``) will be used when evaluating models:","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_training_history(history, title_suffix=\"\"):\n    \"\"\"\n    Retrieves the Keras History object and displays train/val accuracy and loss in a single chart.\n    - history: Keras History object returned after model.fit()\n    - title_suffix: Annotation for the chart title (optional)\n    \"\"\"\n    acc = history.history.get('accuracy', [])\n    val_acc = history.history.get('val_accuracy', [])\n    loss = history.history.get('loss', [])\n    val_loss = history.history.get('val_loss', [])\n\n    if not acc or not val_acc:\n        print(\"Accuracy data not found.\")\n        return\n\n    epochs = range(1, len(acc) + 1)\n\n    plt.figure(figsize=(14,5))\n\n    # Accuracy\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, acc, 'b-', label='Train Accuracy')\n    plt.plot(epochs, val_acc, 'r--', label='Validation Accuracy')\n    plt.title(f'Training & Validation Accuracy {title_suffix}')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.grid(True)\n\n    # Loss\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, loss, 'b-', label='Train Loss')\n    plt.plot(epochs, val_loss, 'r--', label='Validation Loss')\n    plt.title(f'Training & Validation Loss {title_suffix}')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid(True)\n\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## EfficientNetB0","metadata":{}},{"cell_type":"code","source":"train_gen, val_gen = get_generators(TRAIN_MERGE_DIR, VAL_MERGE_DIR, IMG_SIZE, 32, RANDOM_SEED, preprocess_func=effnet_preprocess)\nprint(\"train_generator.class_indices: \", train_gen.class_indices)\nprint(\"steps_per_epoch: \", len(train_gen))\nprint(\"validation_steps:\", len(val_gen))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.applications import EfficientNetV2B0\n\n# Base model\nbase_model = EfficientNetV2B0(weights='imagenet', include_top=False, input_shape=(224,224,3))\nbase_model.trainable = False \n\nmodel_effnet = Sequential([\n    base_model,\n    layers.GlobalAveragePooling2D(),\n    layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n    layers.Dropout(0.5),\n    layers.Dense(1, activation='sigmoid')\n])\n\n# First compile\nmodel_effnet.compile(\n    optimizer=Adam(1e-4),\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\nmodel_effnet.summary()\n\n# Fine tuning\nfor layer in base_model.layers:\n    layer.trainable = False  # turn them all off first\n\nfor layer in base_model.layers[-20:]:  # open the last 20 layers\n    layer.trainable = True\n\n# Recompile (with lower learning rate)\nmodel_effnet.compile(\n    loss='binary_crossentropy',\n    optimizer=Adam(1e-5),\n    metrics=['accuracy']\n)\n\nmodel_effnet.summary()\n\n\nEPOCHS = 30\nes = EarlyStopping(\n    monitor='val_accuracy',\n    mode='max',\n    patience=6,\n    restore_best_weights=True\n)\n\nhistory_effnet = model_effnet.fit(\n    train_gen,\n    steps_per_epoch=len(train_gen),\n    epochs=EPOCHS,\n    validation_data=val_gen,\n    validation_steps=len(val_gen),\n    callbacks=[es]\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Success of the model and results:\n\nWhile the model generalizes well (validation accuracy is ≈88%), the significant gap between the training and validation metrics suggests overfitting is occurring:\n\n| Metric    | Training (Epoch 30) | Validation (Epoch 30) | Difference |\n| -------- | ------- | ------- | ------- |\n| Accuracy | 0.9301 | 0.8822 | 4.79 percentage points |\n| Loss\t   | 0.2258 | 0.2795 | 0.0537 absolute difference |\n\n\nEfficientNetB0 performed with high accuracy and low loss on brain tumor view, but slight overfitting was observed. It also demonstrates that EfficientNetB0 can provide effective feature extraction and classification performance even on small datasets.","metadata":{}},{"cell_type":"code","source":"plot_training_history(history_effnet, title_suffix=\"(EfficientNetB0)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## MobileNetV2","metadata":{}},{"cell_type":"code","source":"train_gen, val_gen = get_generators(TRAIN_MERGE_DIR, VAL_MERGE_DIR, IMG_SIZE, 32, RANDOM_SEED, preprocess_func=mobilenet_preprocess)\nprint(\"train_generator.class_indices: \", train_gen.class_indices)\nprint(\"steps_per_epoch: \", len(train_gen))\nprint(\"validation_steps:\", len(val_gen))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Base Model\nbase_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224,224,3))\nbase_model.trainable = False\n\n# Model\nmodel_mobilenet = Sequential([\n    base_model,\n    layers.GlobalAveragePooling2D(),\n    layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n    layers.Dropout(0.5),\n    layers.Dense(1, activation='sigmoid')\n])\n\n# First compile\nmodel_mobilenet.compile(\n    optimizer=Adam(1e-4),\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\nmodel_mobilenet.summary()\n\n\n# Fine-tuning\nfor layer in base_model.layers:\n    layer.trainable = False  \n\nfor layer in base_model.layers[-20:]: \n    layer.trainable = True\n\n# Recompile with lower LR\nmodel_mobilenet.compile(\n    loss='binary_crossentropy',\n    optimizer=Adam(1e-5),\n    metrics=['accuracy']\n)\n\n# Callbacks\nes = EarlyStopping(\n    monitor='val_accuracy',\n    mode='max',\n    patience=6,\n    restore_best_weights=True\n)\n\n# Fit\nEPOCHS = 30\nhistory_mobilenet = model_mobilenet.fit(\n    train_gen,\n    steps_per_epoch=len(train_gen),\n    epochs=EPOCHS,\n    validation_data=val_gen,\n    validation_steps=len(val_gen),\n    callbacks=[es]\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Success of the model and results:\n\n| Metric    | Training (Epoch 28) | Validation (Epoch 30) | Difference |\n| -------- | ------- | ------- | ------- |\n| Accuracy | 0.9648 | 0.9333 | 3.15 percentage points |\n| Loss\t   | 0.1458 | 0.2924 | 0.1466 absolute difference |\n\nThroughout the training process, the model showed a significant increase in accuracy on both the training and validation sets. By the final epoch, training accuracy reached approximately 96% and validation accuracy reached 93%. The declining curve in the loss function indicates that the training process was stable and that overfitting was controlled. In particular, the lightweight structure of MobileNetV2 enabled faster convergence and higher accuracy on small datasets compared to DenseNet169.\n\nOverall, the fine-tuning approach to pre-trained MobileNetV2 can be considered an effective method for achieving high accuracy with limited data. Model performance can be further improved with data augmentation techniques and learning rate optimizations.","metadata":{}},{"cell_type":"code","source":"plot_training_history(history_mobilenet, title_suffix=\"(MobileNetV2)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## DenseNet169","metadata":{}},{"cell_type":"code","source":"train_gen, val_gen = get_generators(TRAIN_MERGE_DIR, VAL_MERGE_DIR, IMG_SIZE, 32, RANDOM_SEED, preprocess_func=densenet_preprocess)\nprint(\"train_generator.class_indices: \", train_gen.class_indices)\nprint(\"steps_per_epoch: \", len(train_gen))\nprint(\"validation_steps:\", len(val_gen))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Base model\nbase_model = DenseNet169(weights='imagenet', include_top=False, input_shape=(224,224,3))\nbase_model.trainable = False  \n\n# Model\nmodel_densenet = Sequential([\n    base_model,\n    layers.GlobalAveragePooling2D(),\n    layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n    layers.Dropout(0.5),\n    layers.Dense(1, activation='sigmoid')\n])\n\nmodel_densenet.compile(\n    optimizer=Adam(1e-4),\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\nmodel_densenet.summary()\n\n# Fine-tuning\nfor layer in base_model.layers:\n    layer.trainable = False \n\nfor layer in base_model.layers[-20:]:\n    layer.trainable = True\n\nmodel_densenet.compile(\n    loss='binary_crossentropy',\n    optimizer=Adam(1e-5),\n    metrics=['accuracy']\n)\n\n# Callbacks\nes = EarlyStopping(\n    monitor='val_accuracy',\n    mode='max',\n    patience=6,\n    restore_best_weights=True\n)\n\n# Fit\nEPOCHS = 30\nhistory_densenet = model_densenet.fit(\n    train_gen,\n    steps_per_epoch=len(train_gen),\n    epochs=EPOCHS,\n    validation_data=val_gen,\n    validation_steps=len(val_gen),\n    callbacks=[es]\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Success of the model and results:\n\n| Metric    | Training (Epoch 30) | Validation (Epoch 30) | Difference |\n| -------- | ------- | ------- | ------- |\n| Accuracy | 0.9311 | 0.8733 | 5.8 percentage points |\n| Loss\t   | 0.2163 | 0.2910 | 0.747 absolute difference |\n\nThe accuracy values obtained throughout the training process indicate that the model improved steadily on both the training and validation sets. Training accuracy reached approximately 93% and validation accuracy reached 87% in the final epoch. The loss function values also decreased monotonically, with no sudden fluctuations or bursts observed, confirming the training stability.\n\nThe callback has not yet been triggered, as it reached a new optimal value (0.8733) in epoch 30. However, the almost complete absence of improvement over 4 epochs (26–29) indicates that the model is nearing a halt. There is a slight overfitting issue.\n\nOverall, fine-tuning the DenseNet169 pre-trained network can be considered an effective approach to achieving high accuracy with limited data. Further performance improvement is possible through data augmentation techniques, learning rate planning, or regularizing hyperparameter optimization.","metadata":{}},{"cell_type":"code","source":"plot_training_history(history_densenet, title_suffix=\"(DenseNet169)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ResNet50","metadata":{}},{"cell_type":"code","source":"train_gen, val_gen = get_generators(TRAIN_MERGE_DIR, VAL_MERGE_DIR, IMG_SIZE, 32, RANDOM_SEED, preprocess_func=resnet_preprocess)\nprint(\"train_generator.class_indices: \", train_gen.class_indices)\nprint(\"steps_per_epoch: \", len(train_gen))\nprint(\"validation_steps:\", len(val_gen))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Base model\nbase_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224,224,3))\nbase_model.trainable = False \n\n# Model\nmodel_resnet = Sequential([\n    base_model,\n    layers.GlobalAveragePooling2D(),\n    layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n    layers.Dropout(0.5),\n    layers.Dense(1, activation='sigmoid')\n])\n\n# First compile\nmodel_resnet.compile(\n    optimizer=Adam(1e-4),\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\nmodel_resnet.summary()\n\n# Fine-tuning\nfor layer in base_model.layers:\n    layer.trainable = False \n\nfor layer in base_model.layers[-20:]:\n    layer.trainable = True\n\nmodel_resnet.compile(\n    loss='binary_crossentropy',\n    optimizer=Adam(1e-5),\n    metrics=['accuracy']\n)\n\n# Callbacks\nes = EarlyStopping(\n    monitor='val_accuracy',\n    mode='max',\n    patience=6,\n    restore_best_weights=True\n)\n\n# Fit\nEPOCHS = 30\nhistory_resnet = model_resnet.fit(\n    train_gen,\n    steps_per_epoch=len(train_gen),\n    epochs=EPOCHS,\n    validation_data=val_gen,\n    validation_steps=len(val_gen),\n    callbacks=[es]\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Success of the model and results:\n\n\n| Metric    | Training (Epoch 17) | Validation (Epoch 17) | Difference |\n| -------- | ------- | ------- | ------- |\n| Accuracy | 0.9826 | 0.9156 | 6.7 percentage points |\n| Loss\t   | 0.1072 | 0.2625 | 0.1553 absolute difference |\n\nDuring the training process, the model demonstrated a significant increase in accuracy on both the training and validation sets. By the final epoch, training accuracy reached approximately 98% and validation accuracy reached approximately 91%. The model successfully stopped overfitting early with the EarlyStopping mechanism. ResNet50's robust architecture provided higher accuracy and faster convergence on the small dataset compared to MobileNetV2 and DenseNet169. However, the risk of overfitting should be considered.\n\nOverall, fine-tuning the pre-trained ResNet50 can be considered an effective method for achieving high accuracy with limited data. Performance can be further improved with data augmentation, regularizer hyperparameter optimization, and learning rate adjustments.","metadata":{}},{"cell_type":"code","source":"plot_training_history(history_resnet, title_suffix=\"(ResNet50)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## VGG16","metadata":{}},{"cell_type":"code","source":"train_gen, val_gen = get_generators(TRAIN_MERGE_DIR, VAL_MERGE_DIR, IMG_SIZE, 32, RANDOM_SEED, preprocess_func=vgg16_preprocess)\nprint(\"train_generator.class_indices: \", train_gen.class_indices)\nprint(\"steps_per_epoch: \", len(train_gen))\nprint(\"validation_steps:\", len(val_gen))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.applications import VGG16\n\n# Base model\nbase_model = VGG16(weights='imagenet', include_top=False, input_shape=(224,224,3))\nbase_model.trainable = False \n\n# Model\nmodel_vgg16 = Sequential([\n    base_model,\n    layers.GlobalAveragePooling2D(),\n    layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n    layers.Dropout(0.5),\n    layers.Dense(1, activation='sigmoid')\n])\n\n# First compile\nmodel_vgg16.compile(\n    optimizer=Adam(1e-4),\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\nmodel_vgg16.summary()\n\n# Fine-tuning\nfor layer in base_model.layers:\n    layer.trainable = False \n\nfor layer in base_model.layers[-20:]:\n    layer.trainable = True\n\nmodel_vgg16.compile(\n    loss='binary_crossentropy',\n    optimizer=Adam(1e-5),\n    metrics=['accuracy']\n)\n\n# Callbacks\nes = EarlyStopping(\n    monitor='val_accuracy',\n    mode='max',\n    patience=6,\n    restore_best_weights=True\n)\n\n# Fit\nEPOCHS = 30\nhistory_vgg16 = model_vgg16.fit(\n    train_gen,\n    steps_per_epoch=len(train_gen),\n    epochs=EPOCHS,\n    validation_data=val_gen,\n    validation_steps=len(val_gen),\n    callbacks=[es]\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Success of the model and results:","metadata":{}},{"cell_type":"code","source":"plot_training_history(history_vgg16, title_suffix=\"(VGG16)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## InceptionV3","metadata":{}},{"cell_type":"code","source":"train_gen, val_gen = get_generators(TRAIN_MERGE_DIR, VAL_MERGE_DIR, IMG_SIZE, 32, RANDOM_SEED, preprocess_func=inception_preprocess)\nprint(\"train_generator.class_indices: \", train_gen.class_indices)\nprint(\"steps_per_epoch: \", len(train_gen))\nprint(\"validation_steps:\", len(val_gen))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Base model\nbase_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224,224,3))\nbase_model.trainable = False\n\n# Model\nmodel_inception = Sequential([\n    base_model,\n    layers.GlobalAveragePooling2D(),\n    layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n    layers.Dropout(0.5),\n    layers.Dense(1, activation='sigmoid')\n])\n\nmodel_inception.compile(\n    optimizer=Adam(1e-4),\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\nmodel_inception.summary()\n\n# Fine-tuning\nfor layer in base_model.layers:\n    layer.trainable = False  \n\nfor layer in base_model.layers[-20:]:\n    layer.trainable = True\n\nmodel_inception.compile(\n    loss='binary_crossentropy',\n    optimizer=Adam(1e-5),\n    metrics=['accuracy']\n)\n\n# Callbacks\nes = EarlyStopping(\n    monitor='val_accuracy',\n    mode='max',\n    patience=6,\n    restore_best_weights=True\n)\n\n# Fit\nEPOCHS = 30\nhistory_inception = model_inception.fit(\n    train_gen,\n    steps_per_epoch=len(train_gen),\n    epochs=EPOCHS,\n    validation_data=val_gen,\n    validation_steps=len(val_gen),\n    callbacks=[es]\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Success of the model and results:","metadata":{}},{"cell_type":"code","source":"plot_training_history(history_inception, title_suffix=\"(InceptionV3)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Custom CNN","metadata":{}},{"cell_type":"code","source":"train_gen, val_gen = get_generators(TRAIN_MERGE_DIR, VAL_MERGE_DIR, IMG_SIZE, 32, RANDOM_SEED, preprocess_func=None)\nprint(\"train_generator.class_indices: \", train_gen.class_indices)\nprint(\"steps_per_epoch: \", len(train_gen))\nprint(\"validation_steps:\", len(val_gen))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Model\nmodel_custom = Sequential([\n    layers.Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(224,224,3)),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D((2,2)),\n\n    layers.Conv2D(64, (3,3), activation='relu', padding='same'),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D((2,2)),\n\n    layers.Conv2D(128, (3,3), activation='relu', padding='same'),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D((2,2)),\n\n    layers.Conv2D(256, (3,3), activation='relu', padding='same'),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D((2,2)),\n\n    layers.Flatten(),\n    layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n    layers.Dropout(0.5),\n    layers.Dense(1, activation='sigmoid')\n])\n\n# Compile\nmodel_custom.compile(\n    optimizer=Adam(1e-4),\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\nmodel_custom.summary()\n\n# Callbacks\nes = EarlyStopping(\n    monitor='val_accuracy',\n    mode='max',\n    patience=6,\n    restore_best_weights=True\n)\n\n# Fit\nEPOCHS = 30\nhistory_custom_cnn = model_custom.fit(\n    train_gen,\n    steps_per_epoch=len(train_gen),\n    epochs=EPOCHS,\n    validation_data=val_gen,\n    validation_steps=len(val_gen),\n    callbacks=[es]\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Success of the model and results:","metadata":{}},{"cell_type":"code","source":"plot_training_history(history_custom_cnn, title_suffix=\"(CustomCNN)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Updated CNN (Keras Turner with Bayesian Optimization)","metadata":{}},{"cell_type":"code","source":"train_gen, val_gen = get_generators(TRAIN_MERGE_DIR, VAL_MERGE_DIR, IMG_SIZE, 32, RANDOM_SEED, preprocess_func=None)\nprint(\"train_generator.class_indices: \", train_gen.class_indices)\nprint(\"steps_per_epoch: \", len(train_gen))\nprint(\"validation_steps:\", len(val_gen))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_model(hp):\n    model_tuner = Sequential([\n        layers.Conv2D(\n            filters=hp.Choice('conv1_filters', [32, 64]),\n            kernel_size=(3,3),\n            activation='relu',\n            padding='same',\n            input_shape=(224,224,3)\n        ),\n        layers.BatchNormalization(),\n        layers.MaxPooling2D((2,2)),\n\n        layers.Conv2D(\n            filters=hp.Choice('conv2_filters', [64, 128]),\n            kernel_size=(3,3),\n            activation='relu',\n            padding='same'\n        ),\n        layers.BatchNormalization(),\n        layers.MaxPooling2D((2,2)),\n\n        layers.Conv2D(\n            filters=hp.Choice('conv3_filters', [128, 256]),\n            kernel_size=(3,3),\n            activation='relu',\n            padding='same'\n        ),\n        layers.BatchNormalization(),\n        layers.MaxPooling2D((2,2)),\n\n        layers.Flatten(),\n        layers.Dense(\n            units=hp.Choice('dense_units', [128, 256, 512]),\n            activation='relu',\n            kernel_regularizer=regularizers.l2(1e-4)\n        ),\n        layers.Dropout(hp.Choice('dropout_rate', [0.3, 0.5])),\n        layers.Dense(1, activation='sigmoid')\n    ])\n\n    model_tuner.compile(\n        optimizer=Adam(\n            learning_rate=hp.Float('learning_rate', 1e-5, 1e-3, sampling='log')\n        ),\n        loss='binary_crossentropy',\n        metrics=['accuracy']\n    )\n    return model_tuner\n\ntuner = kt.BayesianOptimization(\n    build_model,\n    objective='val_accuracy',\n    max_trials=5,  # different combination\n    directory='tuner_dir',\n    project_name='custom_cnn_bayes'\n)\n\nes = EarlyStopping(\n    monitor='val_accuracy',\n    mode='max',\n    patience=6,\n    restore_best_weights=True\n)\n\ntuner.search(\n    train_gen,\n    steps_per_epoch=len(train_gen),\n    validation_data=val_gen,\n    validation_steps=len(val_gen),\n    epochs=5, \n    callbacks=[es]\n)\n\nbest_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n\nprint(\"Best hyperparameters:\")\nprint(\"Conv1 filters:\", best_hps.get('conv1_filters'))\nprint(\"Conv2 filters:\", best_hps.get('conv2_filters'))\nprint(\"Conv3 filters:\", best_hps.get('conv3_filters'))\nprint(\"Dense units:\", best_hps.get('dense_units'))\nprint(\"Dropout:\", best_hps.get('dropout_rate'))\nprint(\"Learning rate:\", best_hps.get('learning_rate'))\n\n# Final model\nmodel_tuner = tuner.hypermodel.build(best_hps)\nhistory_tuner_bayesian = model_tuner.fit(\n    train_gen,\n    steps_per_epoch=len(train_gen),\n    validation_data=val_gen,\n    validation_steps=len(val_gen),\n    epochs=30, \n    callbacks=[es]\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Success of the model and results:","metadata":{}},{"cell_type":"code","source":"plot_training_history(history_tuner_bayesian, title_suffix=\"(CustomCNNTurnerBayesian)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Results","metadata":{}},{"cell_type":"markdown","source":"### Performance Plot Model","metadata":{}},{"cell_type":"code","source":"histories = {\n    \"EfficientNet\": history_effnet,\n    \"MobileNet\": history_mobilenet,\n    \"DenseNet\": history_densenet,\n    \"ResNet\": history_resnet,\n    \"VGG16\": history_vgg16,\n    \"Inception\": history_inception,\n    \"Custom CNN\": history_custom_cnn,\n    \"Bayesian Tuner\": history_tuner_bayesian\n}\n\n# Validation-Accuracy comparison\nplt.figure(figsize=(12, 6))\nfor name, hist in histories.items():\n    plt.plot(hist.history['val_accuracy'], label=name)\nplt.title(\"Validation-Accuracy comparison\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Validation-Loss comparison\nplt.figure(figsize=(12, 6))\nfor name, hist in histories.items():\n    plt.plot(hist.history['val_loss'], label=name)\nplt.title(\"Validation-Loss Comparison\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Matrix","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\nmodels = {\n    \"EfficientNet\": model_effnet,\n    \"MobileNet\": model_mobilenet,\n    \"DenseNet\": model_densenet,\n    \"ResNet\": model_resnet,\n    \"VGG16\": model_vgg16,\n    \"Inception\": model_inception,\n    \"Custom CNN\": model_custom,\n    \"Bayesian Tuner\": model_tuner\n}\n\nfig, axes = plt.subplots(2, 4, figsize=(20, 10))  # 2 row 4 column (8 model)\naxes = axes.flatten()\n\nfor i, (name, model) in enumerate(models.items()):\n    y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n    cm = confusion_matrix(y_test, y_pred)\n\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, ax=axes[i])\n    axes[i].set_title(name)\n    axes[i].set_xlabel(\"Predicted\")\n    axes[i].set_ylabel(\"True\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\ndef print_classification_reports(models, X_test, y_test, selected_models=None):\n    for name, model in models.items():\n        if selected_models and name not in selected_models:\n            continue\n        \n        y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n        print(f\"\\n--- Classification Report: {name} ---\\n\")\n        print(classification_report(y_test, y_pred, digits=4))\n\nprint_classification_reports(models, X_test, y_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_confusion_matrix(model, X_test, y_test, class_names=[\"0\", \"1\"]):\n    y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n    cm = confusion_matrix(y_test, y_pred)\n    \n    plt.figure(figsize=(5,4))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n    plt.ylabel(\"Actual\")\n    plt.xlabel(\"Predicted\")\n    plt.title(f\"Confusion Matrix\")\n    plt.show()\n\nplot_confusion_matrix(model_custom, X_test, y_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\n\ndef get_img_array(img, target_size=(224,224)):\n    img = cv2.resize(img, target_size)\n    img = img.astype(np.float32) / 255.0\n    img = np.expand_dims(img, axis=0)\n    return img\n\ndef make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n    grad_model = tf.keras.models.Model(\n        [model.inputs],\n        [model.get_layer(last_conv_layer_name).output, model.output]\n    )\n\n    with tf.GradientTape() as tape:\n        conv_outputs, predictions = grad_model(img_array)\n        if pred_index is None:\n            pred_index = tf.argmax(predictions[0])\n        class_channel = predictions[:, pred_index]\n\n    grads = tape.gradient(class_channel, conv_outputs)\n    pooled_grads = tf.reduce_mean(grads, axis=(0,1,2))\n\n    conv_outputs = conv_outputs[0]\n    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n    heatmap = tf.squeeze(heatmap)\n    heatmap = tf.maximum(heatmap, 0) / tf.reduce_max(heatmap)\n    return heatmap.numpy()\n\ndef superimpose_heatmap(img, heatmap, alpha=0.4, colormap=cv2.COLORMAP_JET):\n    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n    heatmap = np.uint8(255 * heatmap)\n    heatmap = cv2.applyColorMap(heatmap, colormap)\n    superimposed_img = cv2.addWeighted(img, 1-alpha, heatmap, alpha, 0)\n    return superimposed_img\n\ntest_img, test_label = val_gen[0][0][0], val_gen[0][1][0]\nlast_conv_layer_name = \"conv2d_3\"\nimg_array = get_img_array(test_img)\nheatmap = make_gradcam_heatmap(img_array, model_custom, last_conv_layer_name)\n\nsuperimposed_img = superimpose_heatmap((test_img*255).astype(np.uint8), heatmap)\nplt.figure(figsize=(6,6))\nplt.imshow(superimposed_img)\nplt.axis('off')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Clean up & Save","metadata":{}},{"cell_type":"code","source":"# clean up the space\n#!rm -rf TRAIN TEST VAL TRAIN_CROP TEST_CROP VAL_CROP\n# save the model\n# model.save('2025-09-25_VGG_model.h5')\nprint(\"DONE\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}